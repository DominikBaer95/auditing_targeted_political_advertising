{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Regression analysis and predictive modeling\n",
    "\n",
    "This file performs the explanatory regression analysis and the predictive modeling."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "236f8bb6e9656d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import pickle\n",
    "from age_gender_distribution_distances import *\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Define some basic variables\n",
    "path_figures = \"../../Doc/WWW24/Figures/\"\n",
    "\n",
    "# Define party order\n",
    "party_order = [\"linke\", \"gruene\", \"spd\", \"fdp\", \"union\", \"afd\"]\n",
    "# Define party labels\n",
    "party_labels = {\"linke\": \"Linke\", \"gruene\": \"Grüne\", \"spd\": \"SPD\", \"fdp\": \"FDP\", \"union\": \"Union\", \"afd\": \"AfD\"}\n",
    "# Define color palette\n",
    "party_pal = {\"spd\": \"#E3000F\", \"union\": \"#000000\", \"gruene\": \"#46962B\", \"fdp\": \"#FFED00\", \"afd\": \"#009EE0\", \"linke\": \"#990099\"}\n",
    "\n",
    "# Party mapping\n",
    "party_mapping = {\n",
    "    \"The Left (Germany)\": \"linke\",\n",
    "    \"Green party\": \"gruene\",\n",
    "    \"Alliance '90/The Greens\": \"gruene\",\n",
    "    \"Social Democratic Party of Germany\": \"spd\",\n",
    "    \"Young Socialists in the SPD\": \"spd\",\n",
    "    \"FDP.The Liberals\": \"fdp\",\n",
    "    \"Free Democratic Party (Germany)\": \"fdp\",\n",
    "    \"CDU/CSU\": \"union\",\n",
    "    \"Christian Democratic Union (Germany)\": \"union\",\n",
    "    \"Christian Social Union in Bavaria\": \"union\",\n",
    "    \"Alternative fÃ¼r Deutschland AfD\": \"afd\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f69a45ff72df861"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "df = pd.read_pickle(\"../../Data/DE_merged_data.pkl\")\n",
    "df[\"ad_duration\"] = df['ad_duration'].dt.days.astype('int16')\n",
    "\n",
    "# Filter relevant data\n",
    "df = df.loc[df[\"party\"] != \"others\"]\n",
    "\n",
    "# Replace False and True with NaN and \"True\"\n",
    "df = df.replace(False, np.nan)\n",
    "df = df.replace(True, \"True\")\n",
    "\n",
    "# Drop columns with only missing values\n",
    "df = df.dropna(axis=1, how=\"all\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "436209a42d346cbe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_efficiency_party = df.groupby(\"party\").mean(\"impressions_per_spending\")[\"impressions_per_spending\"].reset_index()\n",
    "mean_efficiency = mean_efficiency_party[\"impressions_per_spending\"].mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "376f2f26f05f1922"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create age and gender targeting variables\n",
    "#df['targeting_gender_distribution'] = [get_targeting_gender_distribution(row) for _, row in df.iterrows()]\n",
    "df['targeting_age_distribution'] = [get_targeting_age_distribution(row) for _, row in df.iterrows()]\n",
    "\n",
    "# Expand targeting variables to columns\n",
    "#df_gender = df[\"targeting_gender_distribution\"].apply(pd.Series)\n",
    "#df_gender.columns = [\"targeting_\" + c for c in df_gender.columns]\n",
    "\n",
    "df_age = df[\"targeting_age_distribution\"].apply(pd.Series)\n",
    "df_age.columns = [\"targeting_\" + c.replace(\"-\", \"_\").replace(\"+\", \"\") for c in df_age.columns]\n",
    "\n",
    "# Remove original targeting variables\n",
    "df = df.drop(columns=[\"targeting_age_distribution\"])\n",
    "\n",
    "# Merge expanded targeting variables\n",
    "df = pd.concat([df, df_age], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3a8db8c1d4a2ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Variable selection for models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T22:06:25.807574800Z",
     "start_time": "2023-09-26T22:05:51.482373200Z"
    }
   },
   "id": "6286e6d88d7eb11d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select variable names for regression/prediction model\n",
    "\n",
    "# Top 10 targeting categories\n",
    "top10_targeting = [\"interests_exclude_use\", \"interests_include_use\", \"employers_exclude_use\", \"exclude_location\", \"behaviors_exclude_use\", \"include_lookalike\", \"exclude_custom_audience\", \"include_custom_audience\", \"behaviors_include_use\",\"exclude_lookalike\"]\n",
    "\n",
    "# Include categories\n",
    "include_targeting_use = [c for c in df.columns if c.endswith(\"include_use\")]  # used \"include\" category [y/n]\n",
    "include_targeting_count = [c for c in df.columns if c.endswith(\"include_count\")]  # used n criteria for \"include\" category\n",
    "include_other = [c for c in df.columns if c.startswith(\"include\") and \"raw\" not in c and \"location\" not in c]  # lookalike, friend connection, audience data missing, custom\n",
    "\n",
    "# Exclude categories\n",
    "exclude_targeting_use = [c for c in df.columns if c.endswith(\"exclude_use\")]  # used \"exclude\" category [y/n]\n",
    "exclude_targeting_count = [c for c in df.columns if c.endswith(\"exclude_count\")]  # used n criteria for \"exclude\" category\n",
    "exclude_other = [c for c in df.columns if c.startswith(\"exclude\") and \"raw\" not in c]  # lookalike, friend connection, audience data missing, custom\n",
    "\n",
    "# Demographic targeting\n",
    "age_targeting_vars = [c for c in df.columns if c.startswith(\"targeting_\") and not \"17\" in c and \"gender\" not in c and \"age\" not in c]\n",
    "gender_targeting = [\"targeting_gender\"]\n",
    "\n",
    "# Political variables\n",
    "politic_vars = [\"party\", \"candidate_page\"]\n",
    "\n",
    "# Ad variables\n",
    "ad_vars = [\"platform\", \"ad_duration\", \"sentiment_class\", \"weekday_start\", \"weekday_end\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b1a95972f474339"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create new variable summing up targeting criteria\n",
    "df[\"include_count\"] = df[include_targeting_count].sum(axis=1)\n",
    "df[\"exclude_count\"] = df[exclude_targeting_count].sum(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "500e070e7887e16e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanatory regression analysis"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T22:06:34.711031100Z",
     "start_time": "2023-09-26T22:06:34.675185800Z"
    }
   },
   "id": "410b0b5002873eec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define variable groups\n",
    "\n",
    "# Define dependent variable\n",
    "DV = \"impressions_per_spending\"\n",
    "\n",
    "# Categorical variables\n",
    "cat_targeting = include_targeting_use + exclude_targeting_use + include_other + exclude_other # Categorical targeting variables\n",
    "cat_others = politic_vars + [\"platform\", \"weekday_start\", \"weekday_end\", \"sentiment_class\", \"targeting_gender\"] # Other categorical variables\n",
    "\n",
    "# Continuous variables\n",
    "#cont_targeting = include_targeting_count + exclude_targeting_count # Continuous targeting variables\n",
    "cont_others = age_targeting_vars + [\"ad_duration\", \"include_count\", \"exclude_count\"] # Other continuous variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b1ea1f4b4c79d7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reformat exclude_other variables\n",
    "df[exclude_other] = df[exclude_other].replace(\"True\", 1)\n",
    "df[exclude_other] = df[exclude_other].replace(np.nan, 0)\n",
    "df[exclude_other] = df[exclude_other].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f47c2d48f5c1ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Check for variables with no variation and missing values\n",
    "# Define variables to check\n",
    "check_vars = cat_targeting + cat_others + cont_others + [DV]\n",
    "\n",
    "# Check for missing values in variables\n",
    "print(df[check_vars].isna().sum())\n",
    "# => 1906 missing values for sentiment variables\n",
    "\n",
    "# Drop observations with missing values\n",
    "df = df[check_vars].dropna()\n",
    "\n",
    "# Check for variables without variation\n",
    "unique_counts = df[check_vars].nunique()\n",
    "columns_with_one_unique_value = unique_counts[unique_counts == 1].index.tolist()\n",
    "\n",
    "print(\"Columns with only one unique value:\")\n",
    "print(columns_with_one_unique_value)\n",
    "\n",
    "# Exclude variables without variation\n",
    "df = df.drop(columns_with_one_unique_value, axis=1)\n",
    "\n",
    "# Remove variables without variation from variable lists\n",
    "cat_targeting = [c for c in cat_targeting if c not in columns_with_one_unique_value]\n",
    "#cont_targeting = [c for c in cont_targeting if c not in columns_with_one_unique_value]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "374e0a292b9a2e23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dummies for categorical variables\n",
    "\n",
    "# List of categorical variables\n",
    "cat_vars = cat_targeting + cat_others\n",
    "\n",
    "# Encode dummy variables for categorical variables\n",
    "model_df = pd.get_dummies(df, columns=cat_vars, drop_first=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ac575d4586f0861"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Ordinary Least Squares (OLS) regression"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T22:07:28.459195Z",
     "start_time": "2023-09-26T22:07:28.309000400Z"
    }
   },
   "id": "463972e48b6589c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of dummy for targeting variables\n",
    "dummy_targeting = [c + str(\"_1\") for c in cat_targeting]\n",
    "# List of dummies for Top 10 targeting variables\n",
    "dummy_targeting_top10 = [c + str(\"_1\") for c in top10_targeting]\n",
    "# List of dummies for other categorical targeting variables\n",
    "dummy_targeting_others = [c for c in model_df.columns if c not in dummy_targeting and c.startswith(\"include_\") and not c.endswith(\"raw\") and not c.endswith(\"location\") and not c.endswith(\"_0\")]\n",
    "# List of dummies for weekday variables\n",
    "dummy_weekday = [c for c in model_df.columns if c.startswith(\"weekday_\") and not c.endswith(\"_0\") and \"end\" not in c] # Monday is reference category\n",
    "# List of gender dummies\n",
    "dummy_gender = [\"targeting_gender_Men\", \"targeting_gender_Women\"] # Targeting all genders is reference category\n",
    "# List of party dummies\n",
    "dummy_party = [c for c in model_df.columns if c.startswith(\"party_\") and not c.endswith(\"afd\")] # AfD is reference category\n",
    "# List of sentiment dummies\n",
    "dummy_sentiment = [\"sentiment_class_positive\", \"sentiment_class_negative\"] # Neutral sentiment is reference category\n",
    "# List of platform dummies\n",
    "dummy_platform = [\"platform_1\", \"platform_2\"] # 1 = Facebook, 2 = Instagram, 3 = Both platforms (Reference category)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6cc440ef7c50092"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standardize continuous variables\n",
    "cont_std = (model_df[cont_others] - model_df[cont_others].mean()) / model_df[cont_others].std()\n",
    "cont_std.columns = [c + \"_std\" for c in cont_others]\n",
    "model_df = pd.concat([model_df, cont_std], axis=1)\n",
    "cont_others_std = [c + \"_std\" for c in cont_others]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "935b1be72dd014f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define variables for OLS models\n",
    "\n",
    "# Model 1: Targeting variables\n",
    "vars_model_targeting = dummy_targeting_top10 + [\"include_count_std\", \"exclude_count_std\"]\n",
    "\n",
    "# Model 2: Demographic targeting variables\n",
    "vars_model_demographics = dummy_gender + [c + \"_std\" for c in age_targeting_vars]\n",
    "\n",
    "# Model 3: Ad characteristics\n",
    "vars_model_ads = dummy_weekday + dummy_party + dummy_sentiment + dummy_platform + [\"ad_duration_std\", \"candidate_page_1\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5858efffa9031f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to perform OLS regression\n",
    "def regression_analysis(data, variables, filename=None):\n",
    "    # Create formula for OLS model\n",
    "    variables_str = \" + \".join(variables)\n",
    "    y, X = dmatrices(DV + ' ~ ' + variables_str, data=data, return_type='dataframe')\n",
    "    \n",
    "    # Fit model\n",
    "    mod = sm.OLS(y, X)    # Describe model\n",
    "    res = mod.fit(cov_type='HC3')       # Fit model\n",
    "    \n",
    "    # Save results\n",
    "    res.save(\"../../Data/Models/OLS/\" + \"OLS_\" + filename + \".pkl\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(res.summary())\n",
    "    \n",
    "    # Print goodness of fit\n",
    "    print(\"Goodness of fit\")\n",
    "    print(\"N = \"+str(res.nobs))\n",
    "    print(\"R2 = \"+str(res.rsquared))\n",
    "    print(\"R2 adj = \"+str(res.rsquared_adj))\n",
    "    print(\"MSE = \"+str(res.mse_model))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2093367efc908db0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OLS regression for targeting variables\n",
    "models = [vars_model_targeting, vars_model_demographics, vars_model_ads]\n",
    "names = [\"targeting\", \"demographic\", \"ads\"]\n",
    "\n",
    "for model, file in zip(models, names):\n",
    "    regression_analysis(model_df, model, filename=file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a51b9ee52e8d45a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictive modeling"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T21:58:07.331964100Z",
     "start_time": "2023-09-26T21:58:05.665744900Z"
    }
   },
   "id": "62a33b2ef164c377"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define model variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97fa26995d88f80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define independent variables (We have to re-do this because we dropped a reference category for the OLS model)\n",
    "# List of dummy for targeting variables\n",
    "dummy_targeting = [c + str(\"_0\") for c in cat_targeting] + [c + str(\"_1\") for c in cat_targeting]\n",
    "# List of dummies for weekday variables\n",
    "dummy_weekday = [c for c in model_df.columns if c.startswith(\"weekday_start\")] # 0 = Monday, 6 = Sunday\n",
    "# List of gender dummies\n",
    "dummy_gender = [\"targeting_gender_Men\", \"targeting_gender_Women\", \"targeting_gender_All\"]\n",
    "# List of party dummies\n",
    "dummy_party = [c for c in model_df.columns if c.startswith(\"party_\")]\n",
    "# List of sentiment dummies\n",
    "dummy_sentiment = [\"sentiment_class_positive\", \"sentiment_class_neutral\", \"sentiment_class_negative\"]\n",
    "# List of candidate page dummies\n",
    "dummy_candidate_page = [\"candidate_page_0\", \"candidate_page_1\"]\n",
    "# List of platform dummies\n",
    "dummy_platform = [\"platform_1\", \"platform_2\", \"platform_3\"] # 1 = Facebook, 2 = Instagram, 3 = Both platforms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd9690bb4f5d5c46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define independent variables for different models\n",
    "covariates = dummy_weekday + dummy_gender + dummy_party + dummy_sentiment + dummy_candidate_page + dummy_platform + cont_others\n",
    "\n",
    "# Model 1: Categorical targeting variables\n",
    "vars_model_cat = dummy_targeting + covariates"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1835ea898933acd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert party dummies to single party variable\n",
    "model_df[\"party\"] = model_df[dummy_party].idxmax(axis=1).str.replace(\"party_\", \"\")\n",
    "# Count number of observations\n",
    "model_df[\"party\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f3ef665e346754c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set up preprocessing pipeline and models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2a55f66b684aad3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up preprocessing pipeline\n",
    "\n",
    "# Scale numerical variables\n",
    "# Create a ColumnTransformer to specify how to preprocess each column\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), cont_others)  # Scale numerical columns\n",
    "    ],\n",
    "    remainder='passthrough',  # Keep non-numerical columns as they are\n",
    "    verbose_feature_names_out=False)\n",
    "\n",
    "# Create a pipeline to apply the preprocessor\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a08710112e3a892b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up models\n",
    "# Initialize the random forest regressor\n",
    "rf_regressor = RandomForestRegressor()\n",
    "# Initialize the XGBoost regressor\n",
    "xgb_regressor = xgb.XGBRegressor()\n",
    "\n",
    "# Define a grid of hyperparameters to search over\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of boosting rounds\n",
    "    'learning_rate': [0.3, 0.4],  # Step size shrinkage to prevent overfitting\n",
    "    'max_depth': [3, 4, 5],  # Maximum depth of the tree\n",
    "    'subsample': [0.8, 0.9, 1.0],  # Fraction of samples used for fitting trees\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],  # Fraction of features used for fitting trees\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_features': ['sqrt', 'log2'],  # Number of features to consider at each split\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required to be at a leaf node\n",
    "    'bootstrap': [True, False],  # Whether to bootstrap samples when building trees\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b58be36042222b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set up and split data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2f8f8dfe61192b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc15f85ca9aae658"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set number of runs\n",
    "runs = 10"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5c8253ab158727c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create list of seed values\n",
    "seeds = np.random.randint(1, 1000, size=runs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a112bdcd72aa2bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define dependent variable\n",
    "y = model_df[\"impressions_per_spending\"]\n",
    "# Define independent variables\n",
    "X = model_df[vars_model_cat]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68742fffc345a13d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create list with 10 different train/test splits\n",
    "x_train_list = []\n",
    "x_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for seed in seeds:\n",
    "    # Create train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Preprocess data\n",
    "    # Fit the pipeline to the training data\n",
    "    X_train_scaled = pd.DataFrame(pipeline.fit_transform(X_train))\n",
    "    # Apply the pipeline to the test data\n",
    "    X_test_scaled = pd.DataFrame(pipeline.transform(X_test))\n",
    "    \n",
    "    # Assign column names\n",
    "    X_train_scaled.columns = pipeline.get_feature_names_out()\n",
    "    X_test_scaled.columns = pipeline.get_feature_names_out()\n",
    "    \n",
    "    # Append input and results to list\n",
    "    x_train_list.append(X_train_scaled)\n",
    "    x_test_list.append(X_test_scaled)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72a5e8626cd62ee7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to train, fit, and evaluate model\n",
    "def model_training(model, param_grid, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # Create sample weights to handle class imbalance\n",
    "    party_train = x_train[dummy_party].idxmax(axis=1).str.replace(\"party_\", \"\")\n",
    "    weights_train = class_weight.compute_sample_weight(class_weight=\"balanced\", y=party_train)\n",
    "    \n",
    "    # Initialize GridSearchCV for hyperparameter tuning with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=model, \n",
    "                               param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               cv=10,\n",
    "                               n_jobs=30)\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(x_train, y_train, sample_weight=weights_train)\n",
    "\n",
    "    # Get the best hyperparameters from the search\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Use the best model from the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Fit the best model to the training data\n",
    "    best_model.fit(x_train, y_train, sample_weight=weights_train)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    # Compute mean absolute value (MAE)\n",
    "    mae = np.mean(abs(y_test - y_pred))\n",
    "\n",
    "    # Compute the MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Compute the RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Compute the R^2 score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    metrics = {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n",
    "    \n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"R-squared: {r2}\")\n",
    "    \n",
    "    return best_params, best_model, metrics  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a231fecc244917ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train, fit, and evaluate model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27f3c7a96619eb0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up list of results\n",
    "# XGBoost\n",
    "best_params_xgb_list = []\n",
    "best_model_xgb_list = []\n",
    "metrics_xgb_list = []\n",
    "\n",
    "# Random Forest\n",
    "best_params_rf_list = []\n",
    "best_model_rf_list = []\n",
    "metrics_rf_list = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebfc98a6e7a272e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train, fit, and evaluate model for 10 different seeds\n",
    "# XGBoost\n",
    "for run in range(runs):\n",
    "    print(\"Run:\", run)\n",
    "    best_params_xgb, best_model_xgb, metrics_xgb = model_training(xgb_regressor, param_grid_xgb, x_train=x_train_list[run], y_train=y_train_list[run], x_test=x_test_list[run], y_test=y_test_list[run])\n",
    "    \n",
    "    # Append input and results to list\n",
    "    best_params_xgb_list.append(best_params_xgb)\n",
    "    best_model_xgb_list.append(best_model_xgb)\n",
    "    metrics_xgb_list.append(metrics_xgb)\n",
    "    \n",
    "# Save results\n",
    "pickle.dump(best_params_xgb_list, open(\"../../Data/Models/ML/xgb_best_params.pkl\", 'wb'))\n",
    "pickle.dump(best_model_xgb_list, open(\"../../Data/Models/ML/xgb_best_model.pkl\", 'wb'))\n",
    "pickle.dump(metrics_xgb_list, open(\"../../Data/Models/ML/xgb_metrics.pkl\", 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a551b0ea777488c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "for run in range(runs):\n",
    "    print(\"Run:\", run)\n",
    "    best_params_rf, best_model_rf, metrics_rf = model_training(rf_regressor, param_grid_rf, x_train=x_train_list[run], y_train=y_train_list[run], x_test=x_test_list[run], y_test=y_test_list[run])\n",
    "    \n",
    "    # Append input and results to list\n",
    "    best_params_rf_list.append(best_params_rf)\n",
    "    best_model_rf_list.append(best_model_rf)\n",
    "    metrics_rf_list.append(metrics_rf)\n",
    "    \n",
    "# Save results\n",
    "pickle.dump(best_params_rf_list, open(\"../../Data/Models/ML/rf_best_params.pkl\", 'wb'))\n",
    "pickle.dump(best_model_rf_list, open(\"../../Data/Models/ML/rf_best_model.pkl\", 'wb'))\n",
    "pickle.dump(metrics_rf_list, open(\"../../Data/Models/ML/rf_metrics.pkl\", 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "638d75850231e40b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Analysis of residuals"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef63f585daf3eee1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute avg. metrics\n",
    "metrics_xgb_avg = {\"mae\": np.mean([m[\"mae\"] for m in metrics_xgb_list]), \"mse\": np.mean([m[\"mse\"] for m in metrics_xgb_list]), \"rmse\": np.mean([m[\"rmse\"] for m in metrics_xgb_list]), \"r2\": np.mean([m[\"r2\"] for m in metrics_xgb_list])}\n",
    "metrics_rf_avg = {\"mae\": np.mean([m[\"mae\"] for m in metrics_rf_list]), \"mse\": np.mean([m[\"mse\"] for m in metrics_rf_list]), \"rmse\": np.mean([m[\"rmse\"] for m in metrics_rf_list]), \"r2\": np.mean([m[\"r2\"] for m in metrics_rf_list])}\n",
    "\n",
    "# Compute std. metrics\n",
    "metrics_xgb_std = {\"mae\": np.std([m[\"mae\"] for m in metrics_xgb_list]), \"mse\": np.std([m[\"mse\"] for m in metrics_xgb_list]), \"rmse\": np.std([m[\"rmse\"] for m in metrics_xgb_list]), \"r2\": np.std([m[\"r2\"] for m in metrics_xgb_list])}\n",
    "metrics_rf_std = {\"mae\": np.std([m[\"mae\"] for m in metrics_rf_list]), \"mse\": np.std([m[\"mse\"] for m in metrics_rf_list]), \"rmse\": np.std([m[\"rmse\"] for m in metrics_rf_list]), \"r2\": np.std([m[\"r2\"] for m in metrics_rf_list])}\n",
    "\n",
    "# Select best model based on RMSE\n",
    "if metrics_rf_avg[\"rmse\"] < metrics_xgb_avg[\"rmse\"]:\n",
    "    best_model = best_model_rf_list\n",
    "    print(\"Best model: Random forest\")\n",
    "else:\n",
    "    best_model = best_model_xgb_list\n",
    "    print(\"Best model: XGBoost\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a65ac09a361cc1ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute residuals for each run by party\n",
    "residuals = []\n",
    "residuals_avg = []\n",
    "\n",
    "for run in range(runs):\n",
    "    print(\"Run:\", run)\n",
    "    # Compute residuals\n",
    "    residuals_run = pd.Series(y_test_list[run] - best_model[run].predict(x_test_list[run])).reset_index(drop=True)\n",
    "    # Combine residuals with party\n",
    "    party = x_test_list[run][dummy_party].idxmax(axis=1).str.replace(\"party_\", \"\")\n",
    "    residuals_run = pd.concat([residuals_run, party], axis=1)\n",
    "    \n",
    "    # Average residuals per party\n",
    "    residuals_run_avg = residuals_run.groupby(0).mean().reset_index()\n",
    "    residuals_run_avg.columns = [\"party\", \"residuals\"]\n",
    "    \n",
    "    # Append results\n",
    "    residuals.append(residuals_run)\n",
    "    residuals_avg.append(residuals_run_avg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8412fa5b27d6d86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Flatten list of residuals_avg to dataframe with indicator for run\n",
    "residuals = pd.concat(residuals_avg, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Compute mean residuals per party over all runs\n",
    "avg_residuals = residuals.groupby(\"party\").mean().reset_index()\n",
    "# Compute std. residuals per party over all runs\n",
    "std_residuals = residuals.groupby(\"party\").std().reset_index()\n",
    "# Combine mean and std. residuals\n",
    "residuals_plot = pd.merge(avg_residuals, std_residuals, on=\"party\", suffixes=(\"_avg\", \"_std\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d248c5e125212e7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute relative advantage\n",
    "residuals_plot[\"residuals_avg\"] / mean_efficiency"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "665cfd9b8d11197f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "residuals_plot.to_pickle(\"../../Data/Models/ML/residuals_plot.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb3efd6ee16311b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "social_media_targeting",
   "language": "python",
   "display_name": "social_media_targeting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
